{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4643685-adfc-45d1-8163-044fb2b80c19",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "## Setup\n",
    "\n",
    "Requires CUDA and a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3f446-84d0-4ba8-b5f6-54dbf24359eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2f9c7-47d5-471e-9cbb-73fa5d2019f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "cd apex\n",
    "git checkout 810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c \n",
    "pip install -r requirements.txt\n",
    "# Use the commented `pip install` line instead if the pip version < 24\n",
    "# pip install -v --no-build-isolation --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218c1db-7a38-42ca-a50b-6fac4dc8c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex.optimizers.fused_lamb import FusedLAMB\n",
    "import torch\n",
    "from torch.utils.benchmark import Compare, Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias, Timer\n",
    "from transformers import set_seed\n",
    "\n",
    "from pytorch_fused_lamb import Lamb\n",
    "\n",
    "from tests.reference import Lamb as ReferenceLamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de64af3-8bc2-4b57-8200-1ffdc957cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0a0+git1346ebf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22dc89f-f475-47d7-91fa-2bc120d383c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5041749-1131-4f5c-bb08-b6f877b50e1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Description\n",
    "\n",
    "Optimizers update a list of paramters in place. In a naive implementation, the optimizer loops over the list of paramter tensors and performs the update ops for each tensor individually. If the list of parameters is large, this introduces significant overhead since kernel launches are costly. `torch._foreach` allows to fuse this procedure horizontally over the list of parameters. Still, every update op launches its own kernel, which is inefficient. To vertically fuse this, `torch.compile` can be used\n",
    "\n",
    "The following benchmark compares a vertically and horizontally fused implementation of the LAMB optimizer with a reference implementation in PyTorch and the fused CUDA kernel of the nvidia/apex library.\n",
    "\n",
    "Note: currently, torch optimizers do not support the `fullgraph=True`, `mode=\"max-autotune\"` and `mode=\"max-autotune\"` options of `torch.compile` (see https://github.com/pytorch/pytorch/pull/118987). Therefore, the fused LAMB implementation does not get the full benefit of fusion (e.g. kernel launch overhead reduction using CUDA graphs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd232b-563f-4ccd-ae3c-a0a37af1a4c1",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Canonical benchmark code from: https://pytorch.org/tutorials/recipes/compiling_optimizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c58f09-460e-4ce5-888f-5833905821ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "seed = 123\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377cd325-7dd6-4f8f-afd0-85069cef71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(100)]\n",
    ")\n",
    "inputs = torch.rand(8, 1024, device=\"cuda\")\n",
    "output = model(inputs)\n",
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ef7f77-ed00-4831-8a47-89d5a5ad4fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0412 00:05:49.738000 140064798246720 torch/_logging/_internal.py:1016] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    }
   ],
   "source": [
    "opt = Lamb(model.parameters(), lr=1e-3)\n",
    "reference_opt = ReferenceLamb(model.parameters(), lr=1e-3)\n",
    "fused_opt = FusedLAMB(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Let's define a helpful benchmarking function:\n",
    "\n",
    "def benchmark_torch_function_in_microseconds(f, sub_label, *args, **kwargs):\n",
    "    t0 = Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n",
    "        sub_label=sub_label,\n",
    "        description=\"runtime\",\n",
    "    )\n",
    "    return t0.blocked_autorange()\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "\n",
    "reference_runtime = benchmark_torch_function_in_microseconds(reference_opt.step, sub_label=\"reference\")\n",
    "fused_runtime = benchmark_torch_function_in_microseconds(fused_opt.step, sub_label=\"apex\")\n",
    "compiled_runtime = benchmark_torch_function_in_microseconds(fn, sub_label=\"compiled\")\n",
    "\n",
    "\n",
    "compare = Compare([reference_runtime, fused_runtime, compiled_runtime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41f8bd2-7c55-4dd2-904d-8ed4ed7d20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------  -----------]\n",
      "                 |  runtime\n",
      "1 threads: ----------------\n",
      "      reference  |    26.2 \n",
      "      apex       |    10.4 \n",
      "      compiled   |    24.1 \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9a419-1326-4e3b-9e0c-2d8f63b718a4",
   "metadata": {},
   "source": [
    "The results show that the fused implementation is ~ 10% faster than the reference implementation, but more than twice as slow as the nvidia/apex CUDA kernel.\n",
    "\n",
    "## Results with Adam\n",
    "\n",
    "For comparison, the same benchmark is run for a different optimizer (Adam), for which a vertically and horizontally fused PyTorch implementation already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f10a81-6b8f-47d2-a2b9-789cd4ff65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex.optimizers.fused_adam import FusedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "023878ee-92ec-4b90-a908-488ed26fb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "fused_opt = FusedAdam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def fn():\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Let's define a helpful benchmarking function:\n",
    "\n",
    "def benchmark_torch_function_in_microseconds(f, sub_label, *args, **kwargs):\n",
    "    t0 = Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n",
    "        sub_label=sub_label,\n",
    "        description=\"runtime\",\n",
    "    )\n",
    "    return t0.blocked_autorange()\n",
    "\n",
    "\n",
    "# Warmup runs to compile the function\n",
    "for _ in range(5):\n",
    "    fn()\n",
    "\n",
    "reference_runtime = benchmark_torch_function_in_microseconds(opt.step, sub_label=\"reference\")\n",
    "fused_runtime = benchmark_torch_function_in_microseconds(fused_opt.step, sub_label=\"apex\")\n",
    "compiled_runtime = benchmark_torch_function_in_microseconds(fn, sub_label=\"compiled\")\n",
    "\n",
    "\n",
    "compare = Compare([reference_runtime, fused_runtime, compiled_runtime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d2e76c0-1e5b-4e81-9731-15c1f5e39742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------  -----------]\n",
      "                 |  runtime\n",
      "1 threads: ----------------\n",
      "      reference  |    63.5 \n",
      "      apex       |     7.1 \n",
      "      compiled   |     5.9 \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381757cd-be80-44f0-a42b-0e6fb16014ee",
   "metadata": {},
   "source": [
    "Interestingly, for the Adam case, the fused implementation is even faster than the nvidia/apex CUDA kernel. This suggests that the fused LAMB implementation could still be further improved to reach similar results. One big improvement would be to replace the following line once `torch._foreach_where` is available (https://github.com/pytorch/pytorch/issues/117884):\n",
    "\n",
    "```\n",
    "        trust_ratio = tuple(torch.where(torch.isinf(ratio), 1., ratio) for ratio in trust_ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5763e22-4cd0-4c6c-a56a-bd36133def4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
